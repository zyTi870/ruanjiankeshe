<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI器官芯片毒性显微检测平台 - 实验报告</title>
<link rel="stylesheet" href="styles.css">
</head>
<body>
<div class="container">
<header class="report-header">
<div class="header-info-large">中国农业大学</div>
<div class="header-info-large">课程设计</div>
<div class="header-info">（2025-2026学年秋季学期）</div>
<h1 class="report-title">课设题目： AI器官芯片毒性显微检测平台</h1>
<div class="header-info">课程名称： 电子信息系统软件实验</div>
<div class="code-link"><a href="https://github.com/zyTi870/ruanjiankeshe/tree/main/课设代码" target="_blank">📂 查看项目完整代码 (Project Code)</a></div>
</header>
<p>课设背景：</p>
<p>1.1课设介绍</p>
<p>本课程设计面向显微镜/显微相机下的细胞与器官芯片图像分析需求，构建了一套“边缘端采集与推理 + 上位机交互与可视化”的实验系统。系统以高通EB2开发板为边缘计算平台，完成摄像头采集、实时视频推流、深度学习推理服务与传统视觉算法的协同；</p>
<p>上位机（PC）侧基于Streamlit搭建交互式实验控制面板，实现网络连接配置、实时预览、分梯度（0/33/66/100）逐张抓拍、批量上传推理、结果展示与数据留存。在算法层面，系统包含两条互补路线：其一为基于OpenCV的HSV颜色分割与轮廓/椭圆拟合方案，可在复杂背景与轻度光照变化下稳定定位培养皿并生成带几何元数据的“数据快照”；其二为基于YOLO11的多类别器官毒性因子识别模型，完成数据集构建、训练与导出，并在EB2端使用ONNX Runtime实现离线推理与服务化部署。</p>
<p>在工程实现上，系统采用多线程生产者-消费者架构、最新帧缓存与MJPEG（multipart/x-mixed-replace）推流机制，显著提升了端侧实时性与稳定性；同时在联调过程中系统性解决了多Python环境冲突、依赖缺失、存储空间不足、摄像头权限与设备节点识别、端口混淆、模型路径错误等工程问题。</p>
<p>实验结果表明：EB2端可稳定完成显微视频流输出并支持上位机跨网络访问；YOLO11模型在EB2 CPU上对单张1485×985图像的端到端推理耗时约800–1000 ms（约1–1.25 FPS），能够满足以静态图像/小批量图像精准分析为主的应用需求。</p>
<p>1.2研究背景与应用场景</p>
<p>器官芯片（Organ-on-a-Chip）是一类将细胞培养、微流控结构与生理微环境调控集成于微尺度芯片中的新型生物检测平台。相较于传统二维培养皿或离体组织培养，器官芯片能够在更接近体内的微环境中维持细胞的形态与功能表达，从而提高毒性评价与药效评估的可信度。在食品安全与环境毒理等应用场景中，农药残留、生物毒素、非法添加剂等风险因子往往需要快速、低成本且可现场实施的检测手段。然而，传统细胞毒性检测多依赖长周期培养、复杂的实验操作流程以及半定量的显微观察，存在耗时长、对人员与仪器依赖强、成本高且难以规模化等问题。</p>
<div class="image-container"><img src="media/image_rId8.png" alt="实验图片" loading="lazy"></div>
<p>基于上述背景，本课程设计以“AI器官芯片毒性显微检测平台（毒立拍）”为目标，提出并实现一套端云协同的原型系统：在边缘端（高通EB2开发板）完成显微视频采集、实时推流与离线推理服务；在上位机（PC）完成可视化界面、实验流程控制与结果管理。系统通过“传统视觉快速定位 + 深度学习多类别识别”的组合策略，一方面提升端侧实时性与鲁棒性，另一方面实现对器官毒性因子类别的自动判别。</p>
<p>1.3课程设计目标与任务分解</p>
<p>本课程设计目标可以概括为“搭建一个可运行、可演示、可复现的端-云（端-上位机）协同显微视觉分析系统”。为便于工程实现与验收，本报告将总体目标拆解为三大模块，并分别给出设计、实现与验证：</p>
<p>（1）OpenCV实时视觉模块：在显微视频流中稳定检测培养皿等关键目标，要求具备实时性、鲁棒性，并在触发保存时生成带丰富几何信息与时间戳的标注截图。</p>
<p>（2）YOLO11模型训练与部署模块：完成数据集采集与划分、模型训练与评估、模型导出（PyTorch→ONNX），并在EB2端使用ONNX Runtime实现推理与服务化接口。</p>
<p>（3）软件系统与交互界面模块：在EB2端实现MJPEG实时推流与推理服务；在PC端实现Streamlit交互界面，包括连接配置、实时预览、四梯度采集流程管理、图片上传推理与结果展示。同时，本课程设计还需要在联调阶段形成系统化的工程排错记录与复现实验步骤，以满足“课程设计报告严谨合格”的要求。</p>
<p>（4）简易硬件搭建：对于显微镜和EB2系统，简易搭建了一个就便携式的细胞毒性显微检测设备。</p>
<div class="image-container"><img src="media/image_rId10.png" alt="实验图片" loading="lazy"></div>
<div class="image-container"><img src="media/image_rId11.png" alt="实验图片" loading="lazy"></div>
<h1>二．YOLO11训练部署与权重格式转换</h1>
<h2>2.1 数据集构建与标注规范</h2>
<p>深度学习模型的性能上限在很大程度上由数据质量决定。本项目的数据来源为显微镜/显微相机下采集的器官芯片（或细胞培养）图像，目标是对图像中的毒性因子类别进行自动识别。数据集构建主要包含“采集-筛选-裁剪/预处理-标注-划分”五个环节：（1）采集：在尽量一致的显微成像条件下采集多批次图像，同时覆盖不同光照、不同焦距、不同背景纹理及不同细胞形态变化；（2）筛选：剔除明显失焦、过曝、遮挡严重或无有效目标的样本，降低噪声样本比例；（3）裁剪与预处理：根据实验需要可保持原分辨率或等比缩放，确保训练与部署端预处理一致；（4）标注：采用统一类别体系与命名规则，避免同类异名或同名异类；（5）划分：按75:25划分训练集与验证集，并尽量按“实验批次/采集日期”分层，避免同一批次高度相似图像同时出现在训练与验证中导致指标虚高。</p>
<p>结合训练文档，本项目整理的显微图像数据量达到千张级（1000+张），并按75:25划分训练集与验证集；在组织数据时建议同步保留“采集批次/实验日期/梯度编号”等元数据，以便后续进行跨批次泛化评估与误差溯源。</p>
<div class="image-container"><img src="media/image_rId12.jpg" alt="实验图片" loading="lazy"></div>
<h2>2.2 模型选型与训练配置</h2>
<p>本课程设计选用Ultralytics YOLO11m作为预训练基座。选择YOLO系列的原因包括：端到端训练流程成熟、社区资料丰富、推理速度与精度平衡较好、导出ONNX支持完善，且便于在资源受限的边缘端进行工程化部署。训练阶段重点在于：确保训练命令、data.yaml配置、类别名称映射与日志/权重保存策略（best/last）均可复现。</p>
<div class="image-container"><img src="media/image_rId13.png" alt="实验图片" loading="lazy"></div>
<h2>2.3 训练过程监控与指标解读</h2>
<p>训练过程中建议同时关注两类信号：其一为优化过程是否稳定（loss是否持续下降、是否出现NaN或震荡）；其二为泛化能力是否提升（验证集mAP、precision/recall、混淆矩阵等）。显微图像场景下纹理细节复杂且类别差异可能较细微，因此建议在报告中同时给出：训练/验证曲线、代表性检测可视化、易错样本与误差归因分析。</p>
<div class="image-container"><img src="media/image_rId14.png" alt="实验图片" loading="lazy"></div>
<div class="image-container"><img src="media/image_rId15.png" alt="实验图片" loading="lazy"></div>
<h2>2.4 权重格式转换与部署路线选择</h2>
<p>模型训练环境与边缘端部署环境通常存在显著差异。研发阶段倾向使用PyTorch等动态框架，便于调试；而部署阶段需要考虑算子支持、硬件加速与工具链兼容性。本项目最初规划采用“best.pth（PyTorch）→ ONNX → DLC（SNPE）→ EB2端SNPE Runtime推理”的路线，以利用骁龙平台加速能力。但实践中发现EB2设备上的SNPE SDK版本与生成DLC所需工具版本存在兼容性问题，导致DLC链路难以闭环。因此采取务实策略：保留ONNX作为中立中间表示，放弃DLC转换，转而采用跨平台性更好的ONNX Runtime进行推理部署，从而规避特定厂商工具链的版本锁。</p>
<h2>2.5 ONNX导出与ONNX Runtime部署要点</h2>
<div class="image-container"><img src="media/image_rId16.png" alt="实验图片" loading="lazy"></div>
<p>ONNX（Open Neural Network Exchange）是开放的框架无关模型表示格式。将PyTorch模型导出为ONNX后，可直接在多种后端运行。本项目使用Ultralytics export将best.pth导出为best.onnx，并固定输入尺寸为1485×985，以确保训练与部署侧预处理一致、形状推导明确。导出完成后，建议进行一致性验证（PC端与EB2端输出对齐），并对性能进行拆分测量（预处理/推理/后处理），以便定位瓶颈。</p>
<p>在EB2设备CPU上，处理单张1485×985输入图像的端到端平均推理耗时约800–1000 ms（约1–1.25 FPS）。考虑到课程设计更侧重静态图像或小批量图像的精准分析与展示，该性能可满足原型系统的基本可用性要求。若后续升级为连续监测或更高通量场景，则需要引入NPU/GPU加速、量化、模型轻量化或ROI裁剪等优化。运行代码如下：</p>
<h2>2.6 结果分析：置信度偏低的原因与改进建议</h2>
<p>实验观察显示，模型输出边界框置信度普遍偏低，正确检测框多分布在0.3–0.6区间。</p>
<p>可能原因包括：（1）类别边界模糊、形态差异连续导致特征分布重叠；（2）标注噪声与样本不均衡使模型学习到更保守的决策；（3）正则/增强偏强（如weight_decay较大）压低置信度；（4）训练与部署端预处理（缩放、归一化、插值）存在细微差异。改进建议：补充高质量样本与难例再标注；尝试类别重加权或Focal Loss；适度调小weight_decay；部署侧引入置信度校准（温度缩放等）；并对端到端预处理流程做一致性核查。</p>
<div class="image-container"><img src="media/image_rId18.png" alt="实验图片" loading="lazy"></div>
<h1>三．实时摄像调试与细菌识别（显微视觉模块）</h1>
<h2>3.1 摄像头采集基础与设备节点确认</h2>
<p>在Linux系统中，UVC摄像头通常以V4L2（Video4Linux2）设备形式暴露为/dev/videoX节点。由于EB2设备可能同时存在多个视频节点（如内置摄像头、USB摄像头、虚拟设备等），首要任务是准确定位目标摄像头对应的设备号。本项目通过v4l2-ctl与OpenCV双重验证：先列举系统识别到的摄像头列表，再分别尝试打开各节点并输出分辨率/帧率信息，最终确认有效设备为/dev/video3。</p>
<h2>3.2 典型采集问题与工程化解决</h2>
<p>在边缘端部署过程中，摄像头采集是最容易出现“看似简单但最耗时”的环节，常见问题及解决思路如下：（1）权限不足：OpenCV提示无法打开设备，通常是当前用户不在video用户组或设备权限异常，可通过sudo运行或将用户加入video组后重新登录；（2）设备被占用：同一时间只能有一个进程独占某些摄像头，需关闭其他占用程序（如测试脚本、浏览器摄像头页）；（3）分辨率/像素格式不匹配：部分摄像头默认输出MJPG或YUYV，OpenCV读取失败时可使用v4l2-ctl强制设置格式，或在代码中设置CAP_PROP_FOURCC与分辨率；（4）掉帧与延迟堆积：若采集、推理与推流在同一线程串行执行，容易形成帧队列堆积，造成“画面延迟越来越大”。本项目通过多线程与最新帧缓存策略解决该问题。</p>
<h2>3.3 视觉识别模块需求与总体思路</h2>
<p>本项目在传统视觉模块中提出了明确的工程指标：实时识别、鲁棒性、高性能、高准确性，并支持图像手动保存、数据增强与元数据标注。具体而言，系统需要能够从摄像头视频流中实时检测出培养皿（或目标区域），并在触发保存指令时截取画面，在保存图像上标注关键几何信息与实验元数据，为后续数据分析与模型再训练提供便利。</p>
<h2>3.4 生产者-消费者多线程架构（解耦I/O与识别）</h2>
<p>为保证界面响应与识别循环的流畅性，本项目采用生产者-消费者模式对采集与处理进行解耦：生产者线程专职从摄像头读帧（I/O不稳定且可能阻塞），将最新帧写入共享缓存；消费者线程以固定节奏从缓存取“最新帧”进行识别与绘制。该架构的核心收益在于：即便摄像头偶发掉帧或读帧耗时波动，主线程UI与识别循环仍可保持稳定帧率，不会被I/O阻塞拖垮。同时，采用“最新帧覆盖”策略可避免帧队列累积造成的延迟堆积。</p>
<h2>3.5 核心识别流程：HSV颜色分割 + 轮廓筛选 + 椭圆拟合</h2>
<p>在显微场景中，目标区域（如培养皿边界或特征区域）往往具有相对稳定的颜色/亮度分布。基于该先验，本项目核心识别流程采用HSV颜色分割：（1）色彩空间转换：将BGR/RGB图像转换到HSV，使颜色信息与亮度信息相对分离；（2）阈值分割：在H、S、V三个通道上设置阈值区间生成mask，将目标区域从复杂背景中剥离；（3）形态学净化：对mask进行开闭运算、腐蚀/膨胀，消除噪点、填补小孔洞；（4）轮廓查找：在净化后的mask上调用findContours得到候选轮廓；（5）几何筛选：按面积、圆度/长宽比等规则剔除非目标轮廓；（6）椭圆拟合：对通过筛选的轮廓执行fitEllipse获得圆心、长短轴与旋转角；（7）结果封装：将最优目标的几何信息（圆心、半径/轴长、面积、长宽比等）打包返回，并在显示帧叠加绘制。</p>
<p>针对“细菌/菌落”类显微目标，在完成培养皿或有效视野（ROI）定位后，可进一步在ROI内执行定量识别：首先进行光照校正与对比度增强（如CLAHE或背景减除），随后采用自适应阈值/颜色阈值生成候选菌落掩膜，再结合形态学开闭运算去除碎噪，使用连通域分析或轮廓分析提取菌落的数量、面积、等效直径与圆度等统计量。在本课程设计阶段，系统已实现ROI稳定定位与带注释数据快照保存，为后续加入菌落计数、增长曲线分析等功能提供了直接的数据与工程接口。</p>
<h2>3.6 算法沿革与优化历程（从Hough到HSV）</h2>
<div class="image-container"><img src="media/image_rId20.png" alt="实验图片" loading="lazy"></div>
<p>V1.0 - 基于霍夫圆计算的尝试：项目之初，使用霍夫圆意图实现对培养皿的精准识别，该思路为识别圆形物体的经典路径。但在实际运行过程中，由于电子显微镜镜头需要运动，摄像头图像往往难以包括整个培养皿圆，导致识别极为困难，算法几乎失效。处理速度过慢：实时图像检测中最大延迟高达三秒之多，即使使用多线程运行，缩小图像处理等手段仍然收效甚微。</p>
<p>V2.0 - 基于Canny边缘的尝试：更改了识别算法，采用了更“通用”的形状识别思路：灰度化 -> 高斯模糊 -> Canny边缘检测 -> 查找轮廓 -> 椭圆拟合。此方案在处理背景简单、对比度高的图像时表现尚可。失效分析: 当将V1.0应用于提供的真实显微镜图像时，算法完全失效。原因在于背景复杂:显微镜下的背景充满大量细胞纹理，在Canny检测后产生了海量的噪声边缘。对比度不足: 目标（紫色）与背景（黄绿色）在转换为灰度图后，亮度差异可能并不显著，导致Canny无法形成完整、封闭的目标轮廓。</p>
<p>V3.0 - 转向颜色分割: 认识到V2.0的局限后，做出了关键的策略调整。分析发现，目标最稳定、最显著的特征是其“紫色”。因此，将识别核心从基于亮度的边缘检测，转向基于色相的颜色分割。引入HSV色彩空间，通过颜色阈值直接提取目标，从根本上解决了背景干扰问题。</p>
<p>V4.0  - 完善数据注释: 在识别精度满足要求后，对截图功能进行数据补足。将save_snapshot函数的目标从保存原始帧，改为保存带有丰富注释信息的“显示帧”，并设计了详尽的“信息面板”，使项目的实用价值和数据友好性达到了一个新的高度。</p>
<div class="image-container"><img src="media/image_rId21.png" alt="实验图片" loading="lazy"></div>
<h1>四．界面设计与EB2双程序推流的系统集成</h1>
<h2>4.1 软件总体架构：两服务 + 一界面</h2>
<p>为降低耦合、便于调试与提升稳定性，本项目在EB2端采用“两程序/两服务”部署方式：推流服务（stream_server）负责摄像头采集与MJPEG输出；推理服务（infer_server）负责模型加载与批量推理接口。两者解耦的关键价值在于：推流与推理负载可独立扩缩，推理服务重启不会影响推流预览，同时也便于在联调阶段分别定位“采集/推流问题”与“模型/推理问题”。PC端Streamlit界面作为控制中枢，统一对接两个服务，形成面向用户的单一操作入口。</p>
<div class="image-container"><img src="media/image_rId22.png" alt="实验图片" loading="lazy"></div>
<div class="image-container"><img src="media/image_rId23.png" alt="实验图片" loading="lazy"></div>
<h2>2 MJPEG推流原理与实现要点</h2>
<p>在课程设计周期与工程复杂度约束下，推流方案需要兼顾实现成本、跨平台兼容性与调试便利性。常见推流技术包括RTSP、WebRTC、HLS、MJPEG等。综合比较后，本项目选择MJPEG（multipart/x-mixed-replace）作为推流方式，主要理由如下：</p>
<p>（1）实现简单：基于Flask即可实现HTTP持续输出，无需额外的媒体服务器与复杂的信令。</p>
<p>（2）兼容性好：浏览器可直接通过<img>标签展示MJPEG流，PC端Streamlit也可无缝嵌入，避免额外播放器依赖。</p>
<p>（3）易于排错：MJPEG本质是“连续JPEG”，抓包与日志分析容易；当推流中断时也更容易定位是网络、编码还是应用层问题。</p>
<p>（4）性能可控：通过调整分辨率、隔帧输出与JPEG质量参数，可以在画质与带宽之间灵活折中。</p>
<p>需要说明的是，MJPEG的缺点是带宽开销相对较大，不适合超高分辨率与大规模并发；但对于本课程设计的单设备局域网实时预览场景，MJPEG是性价比较高的方案。</p>
<p>MJPEG推流通常通过HTTP响应返回multipart/x-mixed-replace内容类型，服务端不断输出以下结构的数据块：--frame\r\nContent-Type: image/jpeg\r\n\r\n<jpeg bytes>\r\n客户端在收到每个边界frame后，将其作为一帧图像渲染。因此，推流服务的关键是：持续产生JPEG字节流，并保证边界与头部格式正确。在实现上，通常以生成器函数（generator）方式逐帧yield，由Flask将其作为流式响应返回。这种方式可以避免一次性加载大量数据到内存，也符合“持续输出”的语义。</p>
<p>推流服务的实现可分为三层：采集、缓存与输出。（1）采集层：使用cv2.VideoCapture打开摄像头设备节点，按设置的分辨率读取帧。在EB2端常见问题是摄像头ID与/dev/videoX节点不一致，需要用v4l2-ctl --list-devices确认。（2）缓存层：为降低延迟与避免编码阻塞，推流服务通常只保留“最新帧”。可采用后台线程持续采集并更新一个共享变量或队列（maxsize=1）。（3）输出层：将最新帧按需要缩放后编码为JPEG（cv2.imencode），并通过yield输出multipart块。可以设置JPEG质量参数以降低带宽。</p>
<p>为增强稳定性，建议增加异常捕获：当读帧失败时返回提示或触发重连；当客户端断开连接时及时释放资源，避免僵尸连接。</p>
<h2>4.3 推流服务实现：最新帧缓存与稳定性增强</h2>
<p>推流服务的核心实现思路可概括为“三段式”：（1）采集线程：循环从/dev/videoX读取帧，做必要的尺寸调整与去噪后，将最新帧写入共享缓存；（2）编码输出：在HTTP路由/video_feed中，以生成器方式不断读取缓存中的最新帧，执行JPEG编码并按边界输出；（3）健康检查：提供/health接口返回采集状态、最近一帧时间戳等信息，便于PC端或运维脚本判断服务是否正常。此外，在摄像头掉线、读帧失败等异常时，服务应捕获异常并尝试自动重连，避免因一次异常导致整体服务退出。</p>
<h2>4.4 推理服务实现：批量接口、类别映射与错误处理</h2>
<p>推理服务的关键任务是将离线模型能力以“可被界面调用的服务”形式暴露出来。本项目采用/infer_batch接口，支持PC端一次上传多张图片（例如四个梯度：0/33/66/100），服务端按统一预处理流程完成推理并返回JSON结果。为提升可用性，返回结果不仅包含类别ID，还应包含明确的类别名称与置信度，例如：1=对照、2=顺铂、3=左旋肉碱、4=对乙酰氨基酚、5=苯并[a]芘。工程上还需考虑：输入为空、图片损坏、模型未加载、推理异常等情况，均应返回可读的错误码与提示信息，避免PC端出现“无响应”或难以定位的问题。</p>
<h2>4.5 Streamlit界面设计：实验流程的界面化封装</h2>
<p>PC端Streamlit界面承担“把分散的端侧能力整合成可操作流程”的作用。界面设计遵循从实验流程出发的原则，将用户操作收敛为几个核心步骤：（1）连接配置：输入EB2的IP地址与端口，检查推流与推理服务是否在线；（2）实时预览：从MJPEG流中抓取并显示最新帧，同时提供必要的帧率/延迟提示；（3）分梯度抓拍：按实验需要将四个梯度（0/33/66/100）对应的图像分别采集并展示缩略图；（4）批量推理：将四张图像一键上传至/infer_batch，等待返回后在右侧展示分类结果；（5）结果展示：以表格形式列出每张图像的预测类别名称、置信度（以及可选的边界框坐标），并支持导出CSV或保存截图。</p>
<div class="image-container"><img src="media/image_rId24.png" alt="实验图片" loading="lazy"></div>
<div class="image-container"><img src="media/image_rId25.png" alt="实验图片" loading="lazy"></div>
<h2>4.6 端到端联调问题与解决方案汇总</h2>
<p>课程设计的难点往往不在单个算法实现，而在于跨设备、跨网络、跨依赖的端到端闭环。本项目在YOLO部署、摄像头采集与推流、界面调用推理等环节都遇到过典型工程问题。为形成可复用经验，下面按“现象-定位-解决”总结关键问题与解决方案（可按你真实经历继续补充）。</p>
<h1>五．简易硬件架构搭建与实验台架介绍</h1>
<h2>5.1 硬件组成与功能分工</h2>
<p>本课程设计采用“显微采集端 + 边缘计算端 + 上位机显示端”的硬件组合。典型硬件组成包括：（1）显微采集装置：显微镜/便携显微模块 + 摄像头（USB UVC），负责将器官芯片/细胞培养区域成像为视频流；（2）边缘计算平台：高通EB2开发板，负责摄像头采集、推流与推理服务运行；（3）网络与供电：路由器/交换机用于局域网互联，稳定供电与USB线材保证摄像头与EB2可靠运行；（4）上位机（PC）：运行Streamlit界面，完成预览、抓拍、批量推理请求与结果可视化；（5）器官芯片与微流控结构：作为被测载体，为不同毒性因子/浓度梯度提供稳定微环境。</p>
<div class="image-container"><img src="media/image_rId26.jpg" alt="实验图片" loading="lazy"></div>
<div class="image-container"><img src="media/image_rId27.jpg" alt="实验图片" loading="lazy"></div>
<h2>5.2 硬件清单</h2>
<h2>5.3 微流控器官芯片结构要点</h2>
<p>本项目的器官芯片与检测载体强调以下结构与工程特性：（1）连续培养检测芯片：在芯片内维持与数据集一致的微环境，使细胞形态变化与训练数据分布更一致；（2）人字形高效稀释结构：支持对毒性因子进行梯度稀释，便于构建0/33/66/100等多梯度实验组；（3）支持12次并行检测与阵列微槽结构：提升通量，适合批量采集与统计分析；（4）集成标准玻片大小：兼容常见显微镜载物台，降低实验装配成本；（5）简单准确的液体驱动：减少复杂泵控依赖，降低使用门槛。这些设计使系统具备“便携式、全自动、低成本”的产品化潜力，并为后续扩展到农药残留、生物毒素、非法添加剂等风险场景提供基础。</p>
<h2>5.4台架搭建注意事项与经验总结</h2>
<p>台架搭建虽不涉及复杂电路，但对稳定性与可重复性影响显著。建议重点关注：（1）成像稳定：显微镜支架与相机固定要牢靠，避免振动导致图像抖动；（2）光照一致：尽量使用稳定光源或固定曝光参数，减少光照变化对分割/识别的影响；（3）对焦与景深：显微场景景深浅，对焦偏差会显著影响纹理细节与模型输出；（4）线材与供电：USB线材质量与供电稳定性直接影响掉线率；（5）散热与长时间运行：EB2长时间推流/推理可能发热，需保证通风或加散热片；（6）标定与重复性：同一实验流程应尽量保持相同的视场位置与倍率，以提升数据一致性。</p>
<h1>六．总结与未来改进展望</h1>
<h2>6.1 工作总结</h2>
<p>本课程设计围绕“显微采集-边缘推理-界面化展示”的完整闭环，完成了从需求分析、算法选型、数据构建、模型训练与导出、到端侧部署、推流服务化、上位机界面化集成与硬件台架搭建的系统实现。具体成果包括：（1）实现EB2端摄像头采集与MJPEG推流服务，可在PC端稳定预览显微画面；（2）完成YOLO11模型训练与ONNX导出，并在EB2端基于ONNX Runtime实现批量推理服务；（3）实现OpenCV传统视觉模块，用于目标区域定位、数据快照保存与元数据标注；（4）开发Streamlit交互界面，实现四梯度抓拍、批量推理与“类别名称+置信度”的结果展示；（5）在联调过程中积累并沉淀了跨环境部署、摄像头排查、网络服务配置与性能瓶颈定位的工程经验。</p>
<h2>6.2 当前系统的能力边界与不足</h2>
<p>尽管系统已满足课程设计原型的可用性与可演示性，但仍存在若干能力边界：（1）推理性能受限：当前以CPU为主，单张高分辨率图像推理耗时较长，不适合高帧率连续检测；（2）数据规模与泛化：数据集规模与采集条件有限，跨批次/跨光照/跨设备泛化仍需进一步验证；（3）置信度解释性：模型输出置信度偏保守，需结合校准或更系统的误差分析；（4）系统健壮性：虽然已加入健康检查与异常捕获，但在长期运行、网络抖动、USB掉线等极端场景下仍可能出现退化；（5）可解释性不足：当前主要输出类别与置信度，缺少对显微纹理“为何这样判定”的可解释性展示。</p>
<p>6.3遇到的关键问题以及解决方案</p>
<p>6.3.1Flask模块缺失（No module named flask）</p>
<p>现象：在EB2端启动推流/推理服务时，终端报错“No module named flask”，程序无法启动。</p>
<p>原因分析：EB2上存在多套Python解释器（系统Python与Conda/Miniforge），依赖可能被安装到某个环境中，但运行时使用了另一套解释器。此外，若使用sudo安装或切换用户，也可能导致site-packages路径不同。</p>
<p>解决方法：</p>
<p>（1）明确激活目标环境（source …/activate）；</p>
<p>（2）用which python确认解释器路径；</p>
<p>（3）使用python -m pip install flask在同一解释器下安装；</p>
<p>（4）必要时打印sys.path确认依赖路径。</p>
<p>验证方式：重新运行import flask或启动服务，确认不再报错，并能正常监听端口。</p>
<p>6.3.2 pip缺失（/usr/bin/python: No module named pip）</p>
<p>现象：在EB2端执行pip安装依赖时提示pip不存在或“No module named pip”。</p>
<p>原因分析：某些系统Python最小化安装不包含pip；或使用了错误的python解释器（例如/usr/bin/python）。</p>
<p>解决方法：优先使用python -m ensurepip（若支持）或安装python3-pip；在Conda环境下通常自带pip，应确保已激活环境并使用python -m pip。</p>
<p>验证方式：执行python -m pip -V确认pip可用，并能正常安装requests/opencv等依赖。</p>
<p>6.3.3 安装opencv-python-headless空间不足（No space left on device）</p>
<p>现象：在EB2端安装opencv-python-headless或onnxruntime时出现“No space left on device”，安装中断。</p>
<p>原因分析：嵌入式开发板常见存储空间有限，且pip缓存与conda包缓存会占用大量空间；同时日志、历史数据与临时文件也可能持续累积。</p>
<p>解决方法：</p>
<p>（1）检查磁盘空间：df -h；</p>
<p>（2）清理pip缓存：pip cache purge；</p>
<p>（3）清理conda缓存：conda clean -a；</p>
<p>（4）删除不必要的临时文件与旧数据集；</p>
<p>（5）若仍不足，考虑将数据/模型放到外接存储或调整安装策略（使用系统OpenCV替代pip版）。</p>
<p>验证方式：再次安装依赖并成功；启动程序运行cv2版本检查。</p>
<p>6.3.4 摄像头权限问题（Permission denied）</p>
<p>现象：打开/dev/videoX时报Permission denied，VideoCapture无法读取。</p>
<p>原因分析：当前用户不在video用户组，或设备节点权限不足；在某些系统中，需要通过udev规则或组权限解决。</p>
<p>解决方法：</p>
<p>（1）查看设备权限：ls -l /dev/video*；</p>
<p>（2）将用户加入video组：sudo usermod -aG video <user>，重新登录生效；</p>
<p>（3）临时方案可使用sudo运行（不推荐作为长期方案）。</p>
<p>验证方式：使用v4l2-ctl --device=/dev/videoX --all或运行OpenCV读帧测试，确认可正常采集。</p>
<p>6.3.5 摄像头索引错误（Camera index out of range）</p>
<p>现象：程序使用VideoCapture(0/1/2/4)等索引时无法打开摄像头，报Camera index out of range或read失败。</p>
<p>原因分析：OpenCV的索引与/dev/videoX并非总是一一对应，在多摄像头或虚拟设备存在时更容易混淆；另外，设备节点可能随重启变化。</p>
<p>解决方法：</p>
<p>（1）使用v4l2-ctl --list-devices列出设备与对应节点；</p>
<p>（2）逐个测试/dev/video0…/dev/videoN，找到实际可用节点；</p>
<p>（3）在代码中使用明确的设备路径（如cv2.VideoCapture('/dev/video3')）或固定DEVICE_ID=3。</p>
<p>验证方式：启动推流服务，观察/health返回camera_opened=true，且/video_feed能输出画面。</p>
<p>6.3.6 端口混淆导致界面看不到实时画面</p>
<p>现象：EB2端服务已启动，但PC端Streamlit界面无法显示实时画面，表现为一直加载或黑屏。</p>
<p>原因分析：常见原因是将推理端口（5001）误填为推流端口（5000），或推流URL路径写错；也可能是EB2端服务实际监听在127.0.0.1而非0.0.0.0，导致局域网无法访问。</p>
<p>解决方法：</p>
<p>（1）在EB2端用ss -lntp确认5000/5001监听情况；</p>
<p>（2）确保Flask启动时host='0.0.0.0'；</p>
<p>（3）在PC浏览器直接访问http://<EB2_IP>:5000/video_feed验证推流独立可用；</p>
<p>（4）在Streamlit中固定构造推流URL，并在页面上显示当前URL供用户核对。</p>
<p>验证方式：浏览器可直接看到video_feed画面，Streamlit嵌入后可实时刷新。</p>
<p>6.3.7 模型找不到或加载失败（Model not found / best.onnx）</p>
<p>现象：推理服务启动时报Model not found: best.onnx或加载ONNX失败。</p>
<p>原因分析：模型文件路径写死但部署目录不同；模型未上传到EB2；或导出ONNX时生成的文件名与代码不一致。此外，若onnxruntime版本与模型opset不兼容，也可能在加载阶段报错。</p>
<p>解决方法：</p>
<p>（1）在服务启动参数中显式传入模型路径，并在启动时打印绝对路径；</p>
<p>（2）使用ls确认模型文件存在且权限可读；</p>
<p>（3）在PC端先用onnxruntime加载验证，再拷贝到EB2；</p>
<p>（4）若opset不兼容，重新导出或升级onnxruntime。</p>
<p>验证方式：访问/health确认model_loaded=true，并执行一次推理请求得到有效JSON。</p>
<p>6.3.8 Streamlit启动方式错误（missing ScriptRunContext）</p>
<p>现象：在PC端直接运行python pc_app.py后出现missing ScriptRunContext或页面无法正常显示。</p>
<p>原因分析：Streamlit应用必须由streamlit运行器托管，以便建立Web会话、状态管理与脚本重跑机制。直接python运行缺少运行上下文。</p>
<p>解决方法：使用命令streamlit run pc_app.py启动，并确保在正确的虚拟环境中安装streamlit。</p>
<p>验证方式：终端显示Local URL与Network URL，浏览器打开后页面正常渲染并可交互。</p>
<h2>6.4未来改进方向</h2>
<p>面向后续迭代，可从工程化与算法两个方向同步推进：工程化方面：（1）引入SNPE/NNAPI等端侧加速链路，解决DLC转换兼容问题后启用NPU/GPU推理；（2）模型量化与轻量化（如INT8量化、蒸馏到更小的YOLO变体），提升吞吐；（3）ROI裁剪与分辨率自适应，先用OpenCV定位目标区域再送入模型，减少无效像素计算；（4）完善服务治理：加入更完整的日志、指标（FPS、延迟）、自动重启与版本管理；（5）界面增强：增加数据管理（实验编号、批次）、一键导出报告、异常提示与操作引导。算法方面：（1）扩充多批次与多光照数据，并开展难例挖掘与再标注；（2）引入置信度校准与不确定性估计，提高结果可靠性；（3）增加可解释性可视化（如Grad-CAM热力图），辅助理解模型关注区域；（4）探索多任务学习，将“毒性因子类别”与“浓度梯度/形态评分”等联合建模，提升信息利用效率。</p>
<h2>6.5 总结</h2>
<p>总体而言，本项目在课程设计周期内完成了较为完整的端侧显微智能检测原型系统。系统以EB2为核心，形成了可复现的采集推流与推理服务化方案，并通过Streamlit界面将实验流程产品化呈现。我们小组4人通过分工合作，成功实现了基本AI器官芯片毒性显微检测，通过这次课设，我们又一次增强了对Linux系统和OpenCV的使用和拓展，在这过程中遇到很多问题，从网络路由器配置到实时识别，在我们自己训练的YOLO11模型权重因为格式问题无法导入到EB2里面，通过格式转换解决，在许多次EB2推流错误，一次次调整代码和配置文件网络，我们成功一一解决所有问题，实现了在Linux系统里面一个可落地的小项目的完成。</p>
<p>附录A  关键命令清单</p>
<p>说明：本附录用于集中列出本课程设计中最常用、最关键的命令，便于他人复现。。</p>
<p>EB2端查看摄像头设备：v4l2-ctl --list-devices</p>
<p>EB2端查看端口监听：ss -lntp | grep -E '5000|5001'</p>
<p>EB2端检查磁盘空间：df -h</p>
<p>EB2端清理pip缓存：python -m pip cache purge</p>
<p>EB2端清理conda缓存：conda clean -a</p>
<p>EB2端启动推流服务：python stream_server.py</p>
<p>EB2端启动推理服务：python infer_server.py --model /path/to/best.onnx</p>
<p>PC端启动界面：streamlit run pc_app.py</p>
<p>PC端浏览器验证推流：http://<EB2_IP>:5000/video_feed</p>
<p>附录B关键代码及其注释</p>
<p>实现实时图像检测：</p>
<p>调用权重进行检测：</p>
<p>构建的可视化界面：</p>
</div>
</body>
</html>
